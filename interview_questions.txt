list = [4,7,[6,3,2,[7,9,2],4],1,3]

list_2 = []

	for i in list :
		if(type(i) != 'list')
		list_2.append(i)
		elif :
		  for j in i:
		   list_2.appened(j)	
def list_fucn(slef,var):
	if(type
	
list_2 = []
	for i in list :
		if(type(i) != 'list')
		list_2.append(i)
		elif :
		  for j in i:
		   list_2.appened(j)

products = spark.createDataFrame([
    (1, "Apple", "Fruits", 1.00),
    (2, "Banana", "Fruits", 0.50),
    (3, "Carrot", "Vegetables", 0.30),
    (4, "Detergent", "Cleaning", 3.00),
    (5, "Shampoo", "Personal Care", 4.00)
], ["ProductID", "ProductName", "Category", "Price"])

sales = spark.createDataFrame([
    (101, 1, 10, "2025-04-01"),
    (102, 2, 20, "2025-04-02"),
    (103, 4, 5, "2025-04-03")
], ["SaleID", "ProductID", "Quantity", "SaleDate"])
 
Write a PySpark query to find the average sales per product category

df_join = sales.join(products,["ProductID"],how = 'inner')
df_1 = df_1.withColumn("Sales", col(df[""Quantity") * col(["Price"]))

df = df_join.groupBy(df["product category"],Product_Id).\
		agg(avg(Sales)).alias("avg_sal")


my_schema = ''' id int, name string,'''



df_csv = spark.read.format("csv").option("sep","-")\
		.option("schema","my_schema")\
		.load()


-------------------------------------------------------------------------------------------
Table A	Table B
ID	ID
1	5
2	4
null	1
3	2
3	2
5	2



inner join:

1 1   1 1 
2 2   1 1 
2 2   2 2
2 2   2 2
5 5   4 3

=> 5

left join
1 1
2 2
2 2
2 2
null null
5 5
3 null
3 null

=> 8

right join

5 5
null 4
1 1
2 2
2 2
2 2

=> 6



   listt = [10, 20, 4, 45, 99, 99, 45]  [4,10,20,45,45,99]
   list_2 = list.sort()
   list3 = list2.pop()
   len = len(list3)
   list4 = list3[len]

	for i in dic.keys : 


dic = {'a':1.'c':9,'z:6}

dic1 = dic.key()









------------------------------------------


Input
city1 city2 distance
banglore kolkata 200
kolkata banglore 200
chennai banglore 100
banglore chennai 100
banglore pune 100

output
kolkata banglore 200
chennai banglore 100
banglore pune 100

select distinct 
select *, case when ((city1 == city2) or (city2 == city1)) then 1 else 0 as key 
from Input
-------------------------------------------------------------------------

+91 2343454567
+918765654324
+1(876)-876-8976
+44 5677865678


df =spar.

list = [+91 2343454567,+918765654324,+1(876)-876-8976,+44 5677865678]

for i in list :
    if(len(left(i,3)) == 3) :
       cc = left(i,3)  +91
       pn = replace(i,cc,'')
     elif(len(left(i) <= 2):
      cc = left(i,2)
      pn = replace(i,cc,'')


----------------------------------------------------
josn = '{timeStart:2025-08-21T12:32:27.877Z, timeEnd:2025-08-21T12:34:18.377Z, durationMs:110500, datasourceConnectionThrottleTimeMs:36930, 
directQueryConnectionTimeMs:43500, directQueryIterationTimeMs:148, directQueryTotalTimeMs:110045, externalQueryExecutionTimeMs:66535, 
vertipaqJobCpuTimeMs:31, mEngineCpuTimeMs:9453, queryProcessingCpuTimeMs:6094, totalCpuTimeMs:15578, executionDelayMs:0, 
approximatePeakMemConsumptionKB:284858, mEnginePeakMemoryKB:276428, directQueryTimeoutMs:4294967295000, tabularConnectionTimeoutMs:0, 
commandType:Statement, queryDialect:0, directQueryRequestCount:33, directQueryTotalRows:261}'

df = spark.read.format('json').load(json1)

df[imeStart]

dbutil.wigets.get("");



mrn,patient_name,dob,
111,sanjay,1995
222,Phani,1997


 
	    